{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13749280-812c-478a-a234-10e51757dcb9",
   "metadata": {},
   "source": [
    "# DA5401 – Assignment 6\n",
    "**Name:** Anan Madhav T V  \n",
    "**Roll No:** MM22B013\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook outline\n",
    "- Part A: Data Preprocessing and Imputation\n",
    "- Part B: Model Training and Performance Assessment\n",
    "- Part C: Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f236b-07da-4cdf-90ff-137c9935915b",
   "metadata": {},
   "source": [
    "## Part A: Data Preprocessing and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcd896-00a0-4c33-9a06-8e3f0eadf250",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "03a73984-75b5-4749-9756-6ef3d417042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea6a26eb-d50b-49f3-bf9a-5650eac45e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre=pd.read_csv('UCI_Credit_Card.csv')\n",
    "df_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "55cabbdc-19b9-4621-ab51-f688766c7d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   ID                          30000 non-null  int64  \n",
      " 1   LIMIT_BAL                   30000 non-null  float64\n",
      " 2   SEX                         30000 non-null  int64  \n",
      " 3   EDUCATION                   30000 non-null  int64  \n",
      " 4   MARRIAGE                    30000 non-null  int64  \n",
      " 5   AGE                         30000 non-null  int64  \n",
      " 6   PAY_0                       30000 non-null  int64  \n",
      " 7   PAY_2                       30000 non-null  int64  \n",
      " 8   PAY_3                       30000 non-null  int64  \n",
      " 9   PAY_4                       30000 non-null  int64  \n",
      " 10  PAY_5                       30000 non-null  int64  \n",
      " 11  PAY_6                       30000 non-null  int64  \n",
      " 12  BILL_AMT1                   30000 non-null  float64\n",
      " 13  BILL_AMT2                   30000 non-null  float64\n",
      " 14  BILL_AMT3                   30000 non-null  float64\n",
      " 15  BILL_AMT4                   30000 non-null  float64\n",
      " 16  BILL_AMT5                   30000 non-null  float64\n",
      " 17  BILL_AMT6                   30000 non-null  float64\n",
      " 18  PAY_AMT1                    30000 non-null  float64\n",
      " 19  PAY_AMT2                    30000 non-null  float64\n",
      " 20  PAY_AMT3                    30000 non-null  float64\n",
      " 21  PAY_AMT4                    30000 non-null  float64\n",
      " 22  PAY_AMT5                    30000 non-null  float64\n",
      " 23  PAY_AMT6                    30000 non-null  float64\n",
      " 24  default.payment.next.month  30000 non-null  int64  \n",
      "dtypes: float64(13), int64(12)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "df_pre.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c517e0fd-3094-404a-84fe-1cdb19fce1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target columns to use MAR on: ['AGE', 'BILL_AMT1', 'BILL_AMT4']\n",
      "\n",
      "Total rows: 30000, missing per column: 3000 (10%)\n",
      "\n",
      "Saved dataframe with 10% MAR missingness to: UCI_Credit_Card_MAR.csv\n",
      "\n",
      "DataFrame info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   ID                          30000 non-null  int64  \n",
      " 1   LIMIT_BAL                   30000 non-null  float64\n",
      " 2   SEX                         30000 non-null  int64  \n",
      " 3   EDUCATION                   30000 non-null  int64  \n",
      " 4   MARRIAGE                    30000 non-null  int64  \n",
      " 5   AGE                         27000 non-null  float64\n",
      " 6   PAY_0                       30000 non-null  int64  \n",
      " 7   PAY_2                       30000 non-null  int64  \n",
      " 8   PAY_3                       30000 non-null  int64  \n",
      " 9   PAY_4                       30000 non-null  int64  \n",
      " 10  PAY_5                       30000 non-null  int64  \n",
      " 11  PAY_6                       30000 non-null  int64  \n",
      " 12  BILL_AMT1                   27000 non-null  float64\n",
      " 13  BILL_AMT2                   30000 non-null  float64\n",
      " 14  BILL_AMT3                   30000 non-null  float64\n",
      " 15  BILL_AMT4                   27000 non-null  float64\n",
      " 16  BILL_AMT5                   30000 non-null  float64\n",
      " 17  BILL_AMT6                   30000 non-null  float64\n",
      " 18  PAY_AMT1                    30000 non-null  float64\n",
      " 19  PAY_AMT2                    30000 non-null  float64\n",
      " 20  PAY_AMT3                    30000 non-null  float64\n",
      " 21  PAY_AMT4                    30000 non-null  float64\n",
      " 22  PAY_AMT5                    30000 non-null  float64\n",
      " 23  PAY_AMT6                    30000 non-null  float64\n",
      " 24  default.payment.next.month  30000 non-null  int64  \n",
      "dtypes: float64(14), int64(11)\n",
      "memory usage: 5.7 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "bill_cols = [c for c in df_pre.columns if c.startswith('BILL_AMT')]\n",
    "targets = ['AGE'] + ['BILL_AMT1'] + ['BILL_AMT4']\n",
    "print(\"Target columns to use MAR on:\", targets)\n",
    "print()\n",
    "\n",
    "n = len(df_pre)\n",
    "n_miss = int(0.1 * n)\n",
    "print(f\"Total rows: {n}, missing per column: {n_miss} (10%)\")\n",
    "print()\n",
    "\n",
    "df_missing = df_pre.copy()\n",
    "\n",
    "for col in targets:\n",
    "    missing_indices = np.random.choice(df_missing.index, size=n_miss, replace=False)\n",
    "    df_missing.loc[missing_indices, col] = np.nan\n",
    "\n",
    "df_missing.to_csv('UCI_Credit_Card_MAR.csv' , index=False)\n",
    "print(\"Saved dataframe with 10% MAR missingness to:\",'UCI_Credit_Card_MAR.csv')\n",
    "print()\n",
    "\n",
    "print(\"DataFrame info():\")\n",
    "df_missing.info()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab99ac3c-706f-449a-a9ec-ac001c2436ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>280000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>9976.0</td>\n",
       "      <td>17976.0</td>\n",
       "      <td>9477.0</td>\n",
       "      <td>9075.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9976.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>9525.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>628699.0</td>\n",
       "      <td>195969.0</td>\n",
       "      <td>179224.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>188840.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>63699.0</td>\n",
       "      <td>64718.0</td>\n",
       "      <td>65970.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>4042.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>3670.0</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2927.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>140000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>64280.0</td>\n",
       "      <td>67079.0</td>\n",
       "      <td>69802.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26636.0</td>\n",
       "      <td>29197.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>210000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE   AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "38  39    50000.0    1          1         2  25.0      1     -1     -1     -2   \n",
       "39  40   280000.0    1          1         2  31.0     -1     -1      2     -1   \n",
       "40  41   360000.0    1          1         2  33.0      0      0      0      0   \n",
       "41  42    70000.0    2          1         2  25.0      0      0      0      0   \n",
       "42  43    10000.0    1          2         2  22.0      0      0      0      0   \n",
       "43  44   140000.0    2          2         1  37.0      0      0      0      0   \n",
       "44  45    40000.0    2          1         2  30.0      0      0      0      2   \n",
       "45  46   210000.0    1          1         2  29.0     -2     -2     -2     -2   \n",
       "\n",
       "    ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "38  ...        NaN        0.0        0.0     780.0       0.0       0.0   \n",
       "39  ...     9976.0    17976.0     9477.0    9075.0       0.0    9976.0   \n",
       "40  ...   628699.0   195969.0   179224.0   10000.0    7000.0    6000.0   \n",
       "41  ...    63699.0    64718.0    65970.0    3000.0    4500.0    4042.0   \n",
       "42  ...     3576.0     3670.0     4451.0    1500.0    2927.0    1000.0   \n",
       "43  ...    64280.0    67079.0    69802.0    3000.0    3000.0    3000.0   \n",
       "44  ...        NaN    26636.0    29197.0    3000.0    5000.0       0.0   \n",
       "45  ...        0.0        0.0        0.0       0.0       0.0       0.0   \n",
       "\n",
       "    PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "38       0.0       0.0       0.0                           1  \n",
       "39    8000.0    9525.0     781.0                           0  \n",
       "40  188840.0   28000.0    4000.0                           0  \n",
       "41    2500.0    2800.0    2500.0                           0  \n",
       "42     300.0    1000.0     500.0                           0  \n",
       "43    4000.0    4000.0    3000.0                           0  \n",
       "44    2000.0    3000.0       0.0                           0  \n",
       "45       0.0       0.0       0.0                           1  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('UCI_Credit_Card_MAR.csv')\n",
    "df.iloc[38:46]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64627e-c4c6-4f3a-9292-aaa81881f889",
   "metadata": {},
   "source": [
    "### Simple Imputation (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa6269-34f6-47b7-a549-0ce4c83083c5",
   "metadata": {},
   "source": [
    "The median is often used for imputation instead of the mean because it is less affected by outliers and skewed data. The mean can be influenced by extreme values, while the median represents the middle value and is not impacted by unusually high or low numbers. Using the median for missing values helps maintain the distribution of the data and provides a more accurate estimate of central tendency compared to the mean, especially when the data is not symmetric. Median imputation is therefore more stable and reliable for real-world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7fbfd174-9314-4d9f-8ac8-46abc6e33394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values in 'AGE' with median: 34.00\n",
      "Filled missing values in 'BILL_AMT1' with median: 22587.00\n",
      "Filled missing values in 'BILL_AMT4' with median: 19145.50\n",
      "\n",
      "Verifying missing values in df_A after imputation:\n",
      "AGE          0\n",
      "BILL_AMT1    0\n",
      "BILL_AMT4    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_A = df.copy()\n",
    "\n",
    "for col in targets:\n",
    "    median_value = df_A[col].median()\n",
    "    df_A[col] = df_A[col].fillna(median_value)\n",
    "    print(f\"Filled missing values in '{col}' with median: {median_value:.2f}\")\n",
    "\n",
    "print(\"\\nVerifying missing values in df_A after imputation:\")\n",
    "print(df_A[targets].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406012da-024b-4766-8764-614e5f25c6f7",
   "metadata": {},
   "source": [
    "### Regression Imputation (Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c84beb-959d-4431-9fdd-5a3c10de1e3a",
   "metadata": {},
   "source": [
    "Regression imputation assumes that missing values are Missing At Random (MAR), meaning that whether a value is missing depends only on other observed features, not on the value itself. For example, if some ages are missing more often for people with higher credit balances, we can predict the missing ages using other features like BILL_AMT1 or LIMIT_BAL. This allows the regression model to estimate missing values based on relationships with available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7e68a69-83f0-4a51-be78-426513aeddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B = df_pre.copy()\n",
    "\n",
    "missing_mask = df_pre.sample(frac=0.1, random_state=42).index\n",
    "df_B.loc[missing_mask, 'BILL_AMT1'] = np.nan\n",
    "df_C=df_B.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "90dfae41-8a4d-49f4-a9dd-662112c3c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B_notna = df_B[df_B['BILL_AMT1'].notna()]\n",
    "df_B_na = df_B[df_B['BILL_AMT1'].isna()]\n",
    "\n",
    "X_train_B = df_B_notna.drop(columns=['BILL_AMT1', 'default.payment.next.month'])\n",
    "y_train_B = df_B_notna['BILL_AMT1']\n",
    "\n",
    "X_pred_B = df_B_na.drop(columns=['BILL_AMT1', 'default.payment.next.month'])\n",
    "\n",
    "lr_B = LinearRegression()\n",
    "lr_B.fit(X_train_B, y_train_B)\n",
    "\n",
    "pred_B = lr_B.predict(X_pred_B)\n",
    "df_B.loc[df_B['BILL_AMT1'].isna(), 'BILL_AMT1'] = pred_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412dacef-b51e-4d7b-9a3b-74fea8a8a014",
   "metadata": {},
   "source": [
    "### Regression Imputation (Non-Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88124ced-3fb5-4ce4-96d0-9ae07a928f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_C = KNNImputer(n_neighbors=5)\n",
    "df_C_imputed = pd.DataFrame(imputer_C.fit_transform(df_C), columns=df_C.columns)\n",
    "\n",
    "df_C = df_C_imputed.copy()\n",
    "\n",
    "df_C['default.payment.next.month'] = df_C['default.payment.next.month'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7013731-024f-47c6-9313-6d2d1d812272",
   "metadata": {},
   "source": [
    "## Part B: Model Training and Performance Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31320dde-f626-4fe3-92a2-6753f22369d0",
   "metadata": {},
   "source": [
    "#### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "29a149fa-59d1-49da-9d52-1c204fd17f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_D = df.dropna().copy()\n",
    "\n",
    "def split_data(df):\n",
    "    X = df.drop(columns=['default.payment.next.month'])\n",
    "    y = df['default.payment.next.month']\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = split_data(df_A)\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = split_data(df_B)\n",
    "X_train_C, X_test_C, y_train_C, y_test_C = split_data(df_C)\n",
    "X_train_D, X_test_D, y_train_D, y_test_D = split_data(df_D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d5428-a5d2-49d5-afff-3b83489f80e7",
   "metadata": {},
   "source": [
    "#### Classifier Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b071ac10-a1c2-466b-a29d-dc67bd3b7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_A = scaler.fit_transform(X_train_A)\n",
    "X_test_A  = scaler.transform(X_test_A)\n",
    "\n",
    "X_train_B = scaler.fit_transform(X_train_B)\n",
    "X_test_B  = scaler.transform(X_test_B)\n",
    "\n",
    "X_train_C = scaler.fit_transform(X_train_C)\n",
    "X_test_C  = scaler.transform(X_test_C)\n",
    "\n",
    "X_train_D = scaler.fit_transform(X_train_D)\n",
    "X_test_D  = scaler.transform(X_test_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18cc0a-a4fc-40d4-886e-23c5b352c2a1",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "face4099-9be7-4bc5-987d-907ca0ab7e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Classification Report: Model A (Median Imputation) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8178    0.9700    0.8874      4673\n",
      "           1     0.6937    0.2389    0.3554      1327\n",
      "\n",
      "    accuracy                         0.8083      6000\n",
      "   macro avg     0.7557    0.6045    0.6214      6000\n",
      "weighted avg     0.7903    0.8083    0.7698      6000\n",
      "\n",
      "\n",
      "\n",
      "--- Classification Report: Model B (Linear Regression Imputation) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8182    0.9690    0.8872      4673\n",
      "           1     0.6888    0.2419    0.3581      1327\n",
      "\n",
      "    accuracy                         0.8082      6000\n",
      "   macro avg     0.7535    0.6054    0.6226      6000\n",
      "weighted avg     0.7896    0.8082    0.7702      6000\n",
      "\n",
      "\n",
      "\n",
      "--- Classification Report: Model C (KNN Imputation) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8183    0.9694    0.8875      4673\n",
      "           1     0.6918    0.2419    0.3585      1327\n",
      "\n",
      "    accuracy                         0.8085      6000\n",
      "   macro avg     0.7550    0.6056    0.6230      6000\n",
      "weighted avg     0.7903    0.8085    0.7705      6000\n",
      "\n",
      "\n",
      "\n",
      "--- Classification Report: Model D (Listwise Deletion) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8214    0.9768    0.8924      3408\n",
      "           1     0.7554    0.2521    0.3780       968\n",
      "\n",
      "    accuracy                         0.8165      4376\n",
      "   macro avg     0.7884    0.6144    0.6352      4376\n",
      "weighted avg     0.8068    0.8165    0.7786      4376\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_evaluate(X_train, X_test, y_train, y_test, label):\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"--- Classification Report: {label} ---\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"\\n\")\n",
    "\n",
    "train_evaluate(X_train_A, X_test_A, y_train_A, y_test_A, \"Model A (Median Imputation)\")\n",
    "train_evaluate(X_train_B, X_test_B, y_train_B, y_test_B, \"Model B (Linear Regression Imputation)\")\n",
    "train_evaluate(X_train_C, X_test_C, y_train_C, y_test_C, \"Model C (KNN Imputation)\")\n",
    "train_evaluate(X_train_D, X_test_D, y_train_D, y_test_D, \"Model D (Listwise Deletion)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd1f0b-ea8f-4188-8bab-a4b5bdccb64a",
   "metadata": {},
   "source": [
    "## Part C: Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832df3f8-337d-4b1e-bd26-601b0c166368",
   "metadata": {},
   "source": [
    "#### Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50f160da-fb65-4752-87fd-3450d81142aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summary of Model Performance ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3b6f8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3b6f8_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_3b6f8_level0_col1\" class=\"col_heading level0 col1\" >Precision (Class 1)</th>\n",
       "      <th id=\"T_3b6f8_level0_col2\" class=\"col_heading level0 col2\" >Recall (Class 1)</th>\n",
       "      <th id=\"T_3b6f8_level0_col3\" class=\"col_heading level0 col3\" >F1-Score (Class 1)</th>\n",
       "      <th id=\"T_3b6f8_level0_col4\" class=\"col_heading level0 col4\" >Test Set Size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Strategy</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3b6f8_level0_row0\" class=\"row_heading level0 row0\" >Model A (Median)</th>\n",
       "      <td id=\"T_3b6f8_row0_col0\" class=\"data row0 col0\" >0.8083</td>\n",
       "      <td id=\"T_3b6f8_row0_col1\" class=\"data row0 col1\" >0.6937</td>\n",
       "      <td id=\"T_3b6f8_row0_col2\" class=\"data row0 col2\" >0.2389</td>\n",
       "      <td id=\"T_3b6f8_row0_col3\" class=\"data row0 col3\" >0.3554</td>\n",
       "      <td id=\"T_3b6f8_row0_col4\" class=\"data row0 col4\" >6000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b6f8_level0_row1\" class=\"row_heading level0 row1\" >Model B (Linear Reg)</th>\n",
       "      <td id=\"T_3b6f8_row1_col0\" class=\"data row1 col0\" >0.8082</td>\n",
       "      <td id=\"T_3b6f8_row1_col1\" class=\"data row1 col1\" >0.6888</td>\n",
       "      <td id=\"T_3b6f8_row1_col2\" class=\"data row1 col2\" >0.2419</td>\n",
       "      <td id=\"T_3b6f8_row1_col3\" class=\"data row1 col3\" >0.3581</td>\n",
       "      <td id=\"T_3b6f8_row1_col4\" class=\"data row1 col4\" >6000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b6f8_level0_row2\" class=\"row_heading level0 row2\" >Model C (KNN Reg)</th>\n",
       "      <td id=\"T_3b6f8_row2_col0\" class=\"data row2 col0\" >0.8085</td>\n",
       "      <td id=\"T_3b6f8_row2_col1\" class=\"data row2 col1\" >0.6918</td>\n",
       "      <td id=\"T_3b6f8_row2_col2\" class=\"data row2 col2\" >0.2419</td>\n",
       "      <td id=\"T_3b6f8_row2_col3\" class=\"data row2 col3\" >0.3585</td>\n",
       "      <td id=\"T_3b6f8_row2_col4\" class=\"data row2 col4\" >6000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b6f8_level0_row3\" class=\"row_heading level0 row3\" >Model D (Listwise Del)</th>\n",
       "      <td id=\"T_3b6f8_row3_col0\" class=\"data row3 col0\" >0.8165</td>\n",
       "      <td id=\"T_3b6f8_row3_col1\" class=\"data row3 col1\" >0.7554</td>\n",
       "      <td id=\"T_3b6f8_row3_col2\" class=\"data row3 col2\" >0.2521</td>\n",
       "      <td id=\"T_3b6f8_row3_col3\" class=\"data row3 col3\" >0.3780</td>\n",
       "      <td id=\"T_3b6f8_row3_col4\" class=\"data row3 col4\" >4376.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x200bd5e6ad0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "datasets_to_evaluate = {\n",
    "    \"Model A (Median)\": (X_train_A, X_test_A, y_train_A, y_test_A),\n",
    "    \"Model B (Linear Reg)\": (X_train_B, X_test_B, y_train_B, y_test_B),\n",
    "    \"Model C (KNN Reg)\": (X_train_C, X_test_C, y_train_C, y_test_C),\n",
    "    \"Model D (Listwise Del)\": (X_train_D, X_test_D, y_train_D, y_test_D)\n",
    "}\n",
    "\n",
    "for name, data in datasets_to_evaluate.items():\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    \n",
    "    model = LogisticRegression(max_iter=500, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    class_1_metrics = report.get('1', {})\n",
    "    \n",
    "    all_results.append({\n",
    "        'Strategy': name,\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Precision (Class 1)': class_1_metrics.get('precision'),\n",
    "        'Recall (Class 1)': class_1_metrics.get('recall'),\n",
    "        'F1-Score (Class 1)': class_1_metrics.get('f1-score'),\n",
    "        'Test Set Size': y_test.shape[0]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(all_results).set_index('Strategy')\n",
    "\n",
    "print(\"--- Summary of Model Performance ---\")\n",
    "results_df.style.format(\"{:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b81f7d-e0d8-4031-b3c9-5365d1663e3e",
   "metadata": {},
   "source": [
    "### Efficacy Discussion\n",
    "\n",
    "\n",
    "#### Trade-off: Listwise Deletion vs. Imputation\n",
    "\n",
    "From the results, Model D (Listwise Deletion) appears to have the highest accuracy (81.53%) and F1-score for class 1 (0.3764). However, this slight performance boost comes with significant drawbacks:\n",
    "\n",
    "* **Information Loss**: This strategy was tested on a much smaller dataset of 4,376 samples, meaning it discarded over 1,600 rows (more than 25%) from the test set alone. This is a massive loss of potentially valuable information.\n",
    "* **Risk of Bias**: By removing any row with missing data, the model is trained on a \"cleaner,\" but likely biased, subset of the data. If the reasons for missing data are not completely random (e.g., specific demographics are less likely to provide certain information), the model will not generalize well to a real-world population.\n",
    "* **Poor Generalization**: A model trained with listwise deletion is impractical for production. It cannot handle new data that contains missing values, making it far less robust than models built on imputed data.\n",
    "\n",
    "Therefore, even if the metrics for Models A, B, and C are slightly lower, they are conceptually superior because they preserve the entire dataset, leading to a more generalized and less biased model.\n",
    "\n",
    "\n",
    "#### Linear vs. Non-Linear Regression Imputation\n",
    "\n",
    "Results show that Model A (Median), Model B (Linear Regression), and Model C (KNN Regression) all produced identical performance metrics.\n",
    "\n",
    "This suggests that the choice of imputation method for the single column (`BILL_AMT1`) had no discernible impact on the final classification task. The most likely reason is that `BILL_AMT1` is not a dominant feature in predicting default. Whether its missing values are filled with a simple median or a more complex regression-based estimate, its overall influence on the model's predictions is negligible.\n",
    "\n",
    "Because the more advanced regression techniques offered no benefit, the simplest and most computationally efficient method Median Imputation is the most logical choice among the three.\n",
    "\n",
    "\n",
    "#### Conclusion and Recommendation\n",
    "\n",
    "**Recommended Strategy: Median Imputation (Model A)**\n",
    "\n",
    "* **Avoid Listwise Deletion**: Model D is not recommended. Its minor performance gain is an illusion created by testing on a smaller, potentially biased dataset. The severe data loss and lack of real-world robustness make it the least desirable option.\n",
    "* **Prefer Simplicity When Performance is Equal**: Since the complex regression imputation methods (Models B and C) provided no advantage over the simple median imputation (Model A), there's no reason to add their computational and implementation overhead.\n",
    "\n",
    "The final recommendation is to use Median Imputation. It is a robust, fast, and simple strategy that preserves the integrity of the dataset. It avoids the risk of bias from listwise deletion and, in this specific case, proved just as effective as more sophisticated imputation techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
